{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a77cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from collections import deque\n",
    "from statistics import mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730a1954",
   "metadata": {},
   "source": [
    "# 1. Load model and camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c339075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = load_model('mnist_model.keras')\n",
    "cap = cv2.VideoCapture(0 + cv2.CAP_DSHOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6490d59",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e228628",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_model_shape(x28):\n",
    "    \"\"\"\n",
    "    Convert a 28x28 grayscale image to the shape expected by the model.\n",
    "    \"\"\"\n",
    "    x28 = x28.astype(\"float32\") / 255.0\n",
    "    in_shape = model.input_shape  \n",
    "\n",
    "    if len(in_shape) == 2 and in_shape[1] == 784:   # flat input\n",
    "        return x28.reshape(1, 784)\n",
    "    else:                                           # image input\n",
    "        return x28.reshape(1, 28, 28, 1)\n",
    "\n",
    "\n",
    "def preprocess_mnist(roi_bgr):\n",
    "    \"\"\"\n",
    "    Process ROI (region of interest) from webcam frame into MNIST-style 28x28 digit.\n",
    "    Steps:\n",
    "        1.Convert to grayscale\n",
    "        2. Blur + adaptive threshold\n",
    "        3. Find largest contour (digit candidate)\n",
    "        4. Extract, dilate, resize, and center on 28x28 canvas\n",
    "    \"\"\"\n",
    "    # grayscale + blur + threshold\n",
    "    g = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    g = cv2.GaussianBlur(g, (5, 5), 0)\n",
    "    th = cv2.adaptiveThreshold(\n",
    "        g, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY_INV, 11, 2\n",
    "    )\n",
    "\n",
    "    # contour detection\n",
    "    cnts, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not cnts:\n",
    "        return None, {\"reason\": \"no contour\"}\n",
    "\n",
    "    # largest contour\n",
    "    c = max(cnts, key=cv2.contourArea)\n",
    "    x, y, w, h = cv2.boundingRect(c)\n",
    "    if w * h < 300:   # reject tiny blobs\n",
    "        return None, {\"reason\": \"tiny area\"}\n",
    "\n",
    "    # extract + dilate\n",
    "    digit = th[y:y+h, x:x+w]\n",
    "    digit = cv2.dilate(digit, np.ones((3, 3), np.uint8), iterations=1)\n",
    "\n",
    "    # keep aspect ratio, resize longer side to 20\n",
    "    H, W = digit.shape\n",
    "    if H > W:\n",
    "        newH, newW = 20, max(1, int(W * (20.0 / H)))\n",
    "    else:\n",
    "        newW, newH = 20, max(1, int(H * (20.0 / W)))\n",
    "    digit = cv2.resize(digit, (newW, newH), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # paste into 28x28 canvas\n",
    "    canvas = np.zeros((28, 28), dtype=np.uint8)\n",
    "    x0, y0 = (28 - newW) // 2, (28 - newH) // 2\n",
    "    canvas[y0:y0+newH, x0:x0+newW] = digit\n",
    "\n",
    "    # center of mass shift\n",
    "    ys, xs = np.nonzero(canvas)\n",
    "    if len(xs):\n",
    "        cx, cy = xs.mean(), ys.mean()\n",
    "        sx, sy = int(round(14 - cx)), int(round(14 - cy))\n",
    "        M = np.float32([[1, 0, sx], [0, 1, sy]])\n",
    "        canvas = cv2.warpAffine(canvas, M, (28, 28),\n",
    "                                flags=cv2.INTER_NEAREST,\n",
    "                                borderValue=0)\n",
    "\n",
    "    return canvas, {\"area\": w * h}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48433387",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "599a85bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = deque(maxlen=7)   # smoothing buffer\n",
    "CONF = 0.55                 # confidence threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a8cb76",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Main loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3857f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok:\n",
    "        break\n",
    "\n",
    "    # Region of Interest (ROI)\n",
    "    H, W = frame.shape[:2]\n",
    "    bw, bh = 160, 160\n",
    "    x1, y1 = max(0, W//2 - bw//2), max(0, H//2 - bh//2)\n",
    "    x2, y2 = min(W, W//2 + bw//2), min(H, H//2 + bh//2)\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "    # Preprocess\n",
    "    x28, info = preprocess_mnist(roi)\n",
    "\n",
    "    # Visualization setup\n",
    "    disp = frame.copy()\n",
    "    color = (0, 255, 0) if x28 is not None else (0, 0, 255)\n",
    "    cv2.rectangle(disp, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "    # Prediction\n",
    "    text = \"No digit\"\n",
    "    if x28 is not None:\n",
    "        inp = to_model_shape(x28)\n",
    "        preds = model.predict(inp, verbose=0)[0]\n",
    "        cls = int(np.argmax(preds))\n",
    "        prob = float(np.max(preds))\n",
    "        history.append((cls, prob))\n",
    "\n",
    "        # smoothing: most frequent confident class in history\n",
    "        votes = [c for c, p in history if p >= CONF]\n",
    "        if votes:\n",
    "            cls_sm = mode(votes)\n",
    "            prob_sm = max(p for c, p in history if c == cls_sm)\n",
    "            text = f\"Pred: {cls_sm}  Prob: {prob_sm:.2f}\"\n",
    "        else:\n",
    "            text = f\"Pred: ?  Prob: {prob:.2f}\"\n",
    "\n",
    "        # debug window for digit\n",
    "        view = cv2.resize(x28, (140, 140), interpolation=cv2.INTER_NEAREST)\n",
    "        cv2.imshow(\"digit28\", view)\n",
    "\n",
    "    # Display prediction\n",
    "    cv2.putText(disp, text, (20, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n",
    "                (255, 0, 255), 2)\n",
    "    cv2.imshow(\"input\", disp)\n",
    "\n",
    "    # Exit on ESC \n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6046df2",
   "metadata": {},
   "source": [
    "# 5. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0b84984",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6b9aef",
   "metadata": {},
   "source": [
    "This Jupyter Notebook implements a real-time digit recognizer that uses a pre-trained Keras model (mnist_model.keras) to identify handwritten digits captured from a webcam. The script defines a region of interest (ROI) in the camera feed, preprocesses the image within it by converting it to grayscale, applying blur and adaptive thresholding, and then isolates and formats the digit into a 28x28 image. This processed image is fed into the neural network for prediction. The application then overlays the predicted digit and its confidence score onto the live video stream, employing a smoothing technique that averages predictions over a short history to enhance stability. The process runs in a continuous loop, which can be terminated by pressing the 'ESC' key, at which point it releases the camera and closes all windows.\n",
    "The main challenges are the fragility of the image preprocessing, which depends heavily on ideal lighting and backgrounds, and the model's poor generalization to real-world digits that don't resemble the clean MNIST dataset. A key future improvement is to replace the static region of interest with a dynamic object detection model (like YOLO) to find digits anywhere in the frame. This would also enable more advanced capabilities, such as recognizing sequences of numbers or even hand-drawn gestures, especially if the model is retrained on more diverse, real-world data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
